vllmImage: quay.io/modh/vllm:rhoai-2.23-cuda

# Application configuration
app: eligibility-lsd
namespace: eligibility-mcp-llamastack
partOf: eligibility-mcp-llamastack

lsdStorageSize: 5Gi
lsdStorageMountPath: /opt/app-root/.llama/distributions/rh

minioEndpoint: http://minio-service.minio.svc.cluster.local:9000
minioAccessKey: minio
minioSecretKey: minio123
minioBucket: hf-cache

lsdCacheMountPath: /data/.hf_cache

lsdImage: quay.io/opendatahub/llama-stack:odh
lsdPlaygroundImage: quay.io/rh-aiservices-bu/llama-stack-playground:0.2.11
lsdPort: 8321

lsdEmbeddingModel: granite-embedding-125m
lsdEmbeddingDimension: 768
lsdEmbeddingModelProvider: sentence-transformers
lsdChunkSizeInTokens: 256

loaderImage: quay.io/atarazana/rag-loader:0.1.1
loaderDelay: 10
loaderJobRetries: 30

milvusDbPath: /tmp/milvus.db
fmsOchestratorUrl: http://localhost:8080

docsFolder: /data/docs

models:
  # - name: llama-3-2-3b
  #   displayName: Llama 3.2 3B
  #   id: RedHatAI/Meta-Llama-3.2-3B-Instruct
  #   image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct
  #   maxModelLen: '15000' # vllm max model len
  #   maxTokens: '4096' # llama stack vllm max token, default is 4096
  #   tlsVerify: false
  #   externalAccess: true
  #   runtime:
  #     templateName: vllm-gaudi-runtime
  #     templateDisplayName: vLLM Intel Gaudi Accelerator ServingRuntime for KServe
  #     image:  quay.io/modh/vllm:rhoai-2.22-gaudi
  #     resources:
  #       limits:
  #         cpu: '8'
  #         memory: 24Gi
  #       requests:
  #         cpu: '6'
  #         memory: 24Gi
  #   accelerator:
  #     type: habana.ai/gaudi
  #     max: '1'
  #     min: '1'
  #   env:
  #     - name: VLLM_SKIP_WARMUP
  #       value: "true"
  #   args:
  #     - --enable-auto-tool-choice
  #     - --tool-call-parser
  #     - llama3_json
  #     - --chat-template
  #     - /app/data/template/tool_chat_template_llama3.1_json.jinja
  - name: granite-3-3-8b
    displayName: Granite 3.3 8B
    id: ibm-granite/granite-3.3-8b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
    maxModelLen: '15000'
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-gaudi-runtime
      templateDisplayName: vLLM Intel Gaudi Accelerator ServingRuntime for KServe
      image: quay.io/modh/vllm:rhoai-2.22-gaudi
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: habana.ai/gaudi
      max: '1'
      min: '1'
    env:
      - name: VLLM_SKIP_WARMUP
        value: "true"
    args:
      - '--enable-auto-tool-choice'
      - '--tool-call-parser'
      - 'granite'
      - '--chat-template'
      - '/app/data/template/tool_chat_template_granite.jinja'
  - name: granite-3-3-2b-cpu
    displayName: Granite 3.3 2B
    id: ibm-granite/granite-3.3-2b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-2b-instruct
    maxModelLen: '15000'
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: image-registry.openshift-image-registry.svc:5000/kwisniewski-llama/vllm-cpu:latest
      runAsSpecificUser: true
      resources:
        limits:
          cpu: '32'
          memory: 24Gi
        requests:
          cpu: '32'
          memory: 24Gi
    env:
      - name: VLLM_SKIP_WARMUP
        value: "true"
      - name: VLLM_CPU_OMP_THREADS_BIND
        value: "auto"
      - name: VLLM_CPU_NUM_OF_RESERVED_CPU
        value: "8"
      - name: VLLM_CPU_KVCACHE_SPACE
        value: "14"
    args:
      - '--enable-auto-tool-choice'
      - '--tool-call-parser'
      - 'granite'
      - '--chat-template'
      - /workspace/tool_chat_template_granite.jinja
  # - name: llama-3-2-3b-cpu
  #   displayName: Llama 3.2 3B
  #   id: RedHatAI/Meta-Llama-3.2-3B-Instruct
  #   image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct
  #   maxModelLen: '15000' # vllm max model len
  #   maxTokens: '4096' # llama stack vllm max token, default is 4096
  #   tlsVerify: false
  #   externalAccess: true
  #   runtime:
  #     templateName: vllm-gaudi-runtime
  #     templateDisplayName: vLLM Intel Gaudi Accelerator ServingRuntime for KServe
  #     # image:  quay.io/modh/vllm:rhoai-2.22-gaudi
  #     image: image-registry.openshift-image-registry.svc:5000/kwisniewski-llama/vllm-cpu:latest
  #     resources:
  #       limits:
  #         cpu: '32'
  #         memory: 24Gi
  #       requests:
  #         cpu: '32'
  #         memory: 24Gi
  #   env:
  #     - name: VLLM_SKIP_WARMUP
  #       value: "true"
  #     - name: VLLM_CPU_OMP_THREADS_BIND
  #       value: "auto"
  #     - name: VLLM_CPU_NUM_OF_RESERVED_CPU
  #       value: "8"
  #     - name: VLLM_CPU_KVCACHE_SPACE
  #       value: "14"
  #   args:
  #     - --enable-auto-tool-choice
  #     - --tool-call-parser
  #     - llama3_json
  #     - --chat-template
  #     - /workspace/tool_chat_template_llama3.1_json.jinja

mcpServers:
  - id: mcp::eligibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/eligibility-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/eligibility-engine-mcp-rs:1.0.4
    mcp_transport: "sse"
    protocol: "http"
    host: eligibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
    env:
      - name: RUST_LOG
        value: "debug"
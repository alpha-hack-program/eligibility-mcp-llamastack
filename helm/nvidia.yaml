vllmImage: quay.io/modh/vllm:rhoai-2.23-cuda

# Application configuration
app: eligibility-lsd
namespace: eligibility-mcp-llamastack
partOf: eligibility-mcp-llamastack

lsdName: rh-dev
# lsdImage: quay.io/opendatahub/llama-stack:odh
lsdImage: quay.io/opendatahub/llama-stack:rhoai-v2.23-latest
lsdBaseDir: /opt/app-root/src
lsdPort: 8321

lsdEmbeddingModel: granite-embedding-125m
lsdEmbeddingDimension: 768
lsdEmbeddingModelProvider: sentence-transformers
lsdChunkSizeInTokens: 256

lsdPlaygroundImage: quay.io/rh-aiservices-bu/llama-stack-playground:0.2.11

loaderImage: quay.io/atarazana/rag-loader:0.1.1
loaderDelay: 10
loaderJobRetries: 30

milvusDbPath: /tmp/milvus.db
fmsOchestratorUrl: http://localhost:8080

docsFolder: /data/docs

models:
  - name: granite-3-3-8b
    displayName: Granite 3.3 8B
    id: ibm-granite/granite-3.3-8b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
    maxModelLen: '15000'
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.23-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: nvidia.com/gpu
      max: '1'
      min: '1'
      # productName: NVIDIA-A10G
    args:
      - '--enable-auto-tool-choice'
      - '--tool-call-parser'
      - 'granite'
      - '--chat-template'
      - '/app/data/template/tool_chat_template_granite.jinja'
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    id: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '15000' # vllm max model len
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.23-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: nvidia.com/gpu
      max: '1'
      min: '1'
      # productName: NVIDIA-A10G-SHARED
      # productName: T4-SHARED
    env:
      - name: VLLM_SKIP_WARMUP
        value: "true"
    args:
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /app/data/template/tool_chat_template_llama3.1_json.jinja

mcpServers:
  - id: mcp::eligibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/eligibility-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/eligibility-engine-mcp-rs:1.0.6
    mcp_transport: "sse"
    protocol: "http"
    host: eligibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 128Mi
    env:
      - name: RUST_LOG
        value: "debug"
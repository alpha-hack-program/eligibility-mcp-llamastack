ðŸŽ‰ Your vLLM + Llama Stack-powered Demo (**{{ .Values.app }}**) has been successfully deployed!

You can access it from project **{{ .Release.Namespace }}**

Models deployed:
{{- range .Values.models }}
- {{ .name }}:
    - URL: "{{ "http://" }}{{ .name }}-predictor:8080/v1"
    - MODEL: "{{ .name }}"
    - IMAGE: "{{ .image }}"
    - RUNTIME: "{{ .runtime.image }}"
  {{- if .api_key }}
    - API_KEY:: "{{ .api_key }}"
  {{- end }}
  {{- if .tlsVerify }}
    - TLS_VERIFY: "{{ .tlsVerify }}"
  {{- end }}
  {{- if .maxTokens }}
    - MAX_TOKENS: "{{ .maxTokens }}"
  {{- end }}
{{- end }}

MCP Servers deployed:
{{- range .Values.mcpServers }}
- {{ .id }}:
    - Git: {{ .vcs.uri }} / {{ .vcs.ref }}
    - Runtime: {{ .runtime }}
    - URI: uri: {{ .protocol }}://{{ .host }}:{{ .port }}{{ .uri }}
    {{- if .env }}
    - Environment variables:
          {{- range .env }}
          - {{ .name }}: {{ .value }}
          {{- end }}
    {{- end }}

{{- end }}

